# _Chapter1：NLP介绍_
&emsp;&emsp;自然语言处理(NLP)是当今世界理解和处理海量非结构化数据的重要工具。最近，深度学习已被广泛用于许多NLP任务，因为深度学习算法在诸如图像分类，语音识别和现实文本生成等过多的具有挑战性的任务中表现出显着的性能。反过来，TensorFlow是目前最直观，最有效的深度学习框架之一。本书将使有抱负的深度学习开发人员能够使用NLP和TensorFlow处理大量数据。 
&emsp;&emsp;在本章中，我们将介绍NLP以及本书的其余部分。我们将回答“什么是自然语言处理？”这一问题。此外，我们将介绍一些最重要的用途。我们还将考虑传统方法和最近基于深度学习的NLP方法，包括**全连接神经网络（FCNN）**。最后，我们将概述本书的其余部分以及我们将使用的技术工具。
## NLP是什么
&emsp;&emsp;根据IBM的数据，2017年每天都会生成2.5艾字节（1 exabyte = 1,000,000,000千兆字节）的数据，随着本书的编写，这种情况也在不断增加。从这个角度来看，如果世界上所有人都要处理这些数据，我们每个人每天处理的数据大约为300 MB。在所有这些数据中，很大一部分是非结构化文本和语音，因为每天都会创建数百万封电子邮件和社交媒体内容以及拨打电话。
&emsp;&emsp;这些统计数据为我们定义NLP提供了良好的基础。简而言之，NLP的目标是让机器理解我们的说出来的和写下来的语言。此外，NLP无处不在，已经成为人类生活的重要组成部分。**虚拟助手（VAs）**，例如Google Assistant，Cortana和Apple Siri，主要是NLP系统。当有人问VA时，会发生许多NLP任务，“你能告诉我一家附近不错的意大利餐馆吗？”首先，VA需要将话语转换为文本（即语音到文本）。接下来，它必须理解请求的语义（例如，用户正在寻找带有意大利美食的好餐厅）并制定结构化请求（例如，美食=意大利语，评级= 3-5，距离<10公里）。然后，VA必须搜索按位置和菜肴过滤的餐馆，然后按收到的评级对餐馆进行排序。为了计算餐馆的整体评级，一个优秀的NLP系统可以查看每个用户提供的评级和文本描述。最后，一旦用户在餐厅，VA可以通过将各种菜单项从意大利语翻译成英语来帮助用户。这个例子表明NLP已成为人类生活中不可或缺的一部分。  
&emsp;&emsp;应该理解，NLP是一个极具挑战性的研究领域，因为单词和语义具有高度复杂的非线性关系，并且将该信息捕获为鲁棒的数值表示更加困难。更糟糕的是，每种语言都有自己的语法，句式和词汇。因此，处理文本数据涉及各种复杂的任务，例如文本解析（例如，分词和词干提取）、词态分析、词义消歧、以及理解语言的基础语法结构。例如，在这两句话中，I went to the bank and I walked along the river bank，bank这个词有两个完全不同的含义。为了区分或（消除歧义）bank这个单词，我们需要理解单词的使用环境。机器学习已成为NLP的关键推动因素，有助于通过机器完成上述任务。
## NLP干什么
&emsp;&emsp;NLP拥有众多实际应用。一个好的NLP系统是执行许多NLP任务的系统。当你在Google上搜索今天的天气或使用谷歌翻译找出如何用法语说说“你好吗？”，你依赖NLP中的此类任务的子集。我们将在这里列出一些最基础的任务，本书涵盖了这些任务的大部分：  
* **分词**：分词是将文本集分离为原子单元（例如，单词）的任务。 虽然看似微不足道，但是分词是一项重要任务。 例如，在日语中，单词不以空格或标点符号分隔。
* **语义消歧（WSD）**：WSD的任务是识别单词的正确含义。例如，在句子中，The dog barked at the mail man, and Tree bark is sometimes used as a medicine，bark这个词有两个不同的含义。WSD对于诸如问答之类的任务至关重要。
* **命名实体辨别（NER）**：NER尝试从给定的文本主体或文本语料库中提取实体（例如，人，位置和组织）。例如，约翰在星期一在学校给玛丽两个苹果的句子，将转换为 ***约翰***（名字）在 ***星期一***（时间）给 ***学校***（组织）的 ***玛丽***（名字） ***两个***（数字）苹果。NER是信息检索和知识表达等领域的必修课。
* **词性（PoS）标注**：PoS标记是将单词标注各自词性的任务。它既可以是名词，动词，形容词，副词，介词等基本标签，也可以是专有名词，普通名词，短语动词，动词等。
* **句子概要分类**：句子或概要（例如，电影评论）分类具有许多用例，例如垃圾邮件检测，新闻文章分类（例如，政治，技术和体育）和产品评论评级（即正面或负面）。这是通过训练具有标记数据的分类模型（即，由人标注的具有正面或负面标签的评论）来实现的。
* **语言生成**：在语言生成中，学习模型（例如，神经网络）使用文本语料库（大量文本文档）进行训练，其预测随后的新文本。例如，语言生成可以通过使用现有的科幻故事进行训练来输出一个全新的科幻故事。
* **问答（QA）**：QA技术具有很高的商业价值，这些技术是聊天机器人和虚拟助手（例如Google Assistant和Apple Siri）的基础。许多公司已经采用聊天机器人来提供客户支持。聊天机器人可用于回答和解决直接的客户问题（例如，更改客户的每月移动计划），无需人工干预即可解决。 QA涉及NLP的许多其他方面，例如信息检索和知识表示。因此，所有这些都使得开发QA系统变得非常困难。
* **机器翻译（MT）**：MT是将句子/短语从源语言（例如，德语）转换为目标语言（例如，英语）的任务。 这是一项非常具有挑战性的任务，因为不同的语言具有高度不同的形态结构，这意味着它不是一对一的转换。 此外，语言之间的单词到单词关系可以是一对多，一对一，多对一或多对多。这被称为MT文献中的单词对齐问题。  
  
&emsp;&emsp;最后，为了开发一个可以帮助人们完成日常任务的系统（例如，VA或聊天机器人），许多这些任务需要一起执行。正如我们在上一个例子中看到的那样，用户问：“你能告诉我附近有一家不错的意大利餐馆吗？” 需要完成几个不同的NLP任务，例如语音到文本转换，语义和情感分析，问题回答和机器翻译。 在图1.1中，我们给出了不同类型的NLP任务的层级分类。我们首先有两大类：分析（分析现有文本）和生成（生成新文本）任务。 然后我们将分析分为三个不同的类别：句法（基于语言结构的任务），语义（基于意义的任务）和务实（难以解决的开放问题）：
![image](https://github.com/jiaojunming/Natural-Language-Processing-with-TensorFlow-CN/blob/master/image/ch1_1.jpg)
  
&emsp;&emsp;了解了NLP中的各种任务后，让我们继续了解如何在机器的帮助下解决这些任务。  
## NLP的经典解法
&emsp;&emsp;解决NLP的传统或经典方法是几个关键步骤的顺序流程，它是一种统计方法。当我们仔细研究传统的NLP学习模型时，我们将能够看到一系列不同的任务，例如通过删除不需要的数据来预处理数据，使用特征工程来获得文本数据的良好数值表示，学习通过训练数据使用机器学习算法，并预测新的不熟悉的数据。 其中，特征工程是在给定NLP任务上获得良好表现的最耗时且最关键的步骤。
### 理解经典解法
&emsp;&emsp;解决NLP任务的经典方法涉及一组不同的子任务。首先，需要对文本语料库进行预处理，重点是减少词汇量和 _干扰_。_干扰_ ，指的是干扰算法捕获任务所需的重要语言信息的东西（例如，标点符号和停止词删除）。  
&emsp;&emsp;接下来，介绍几个功能工程步骤。特征工程的主要目标是使算法的学习更容易。通常，这些特征是手工设计的，并且偏向于人类对语言的理解。特征工程对于经典NLP算法非常重要，因此，效果最佳的系统通常具有最佳工程特征。例如，对于情感分类任务 ，您可以使用解析树表示一个句子，并为树中的每个节点/子树分配正，负或中性标签，以将该句子分类为正或负。 此外，特征工程阶段可以使用外部资源（如WordNet（词汇数据库））来开发更好的特征。我们很快就会看到一种简单的特征工程技术，称为**词袋**。  
&emsp;&emsp;接下来，学习算法使用所获得的特征和可选的外部资源来学习在给定任务中表现良好。例如，对于文本摘要任务，包含单词同义词的同义词库可以是良好的外部资源。最后，预测发生。预测非常简单，提供新的输入并获得预测标签。经典方法的整个过程如图1.2所示：
![image](https://github.com/jiaojunming/Natural-Language-Processing-with-TensorFlow-CN/blob/master/image/ch1_2.jpg)
#### 例子：生成足球比赛摘要
&emsp;&emsp;为了深入理解传统的NLP方法，让我们考虑从足球比赛的统计数据中自动生成文本的任务。我们有几部分比赛统计数据（例如，得分，罚分和黄牌）以及由记者为比赛生成的相应文章作为训练数据。我们还假设对于给定的比赛，我们有从每个统计参数到该参数的摘要的最相关短语的映射。我们的任务是，在新比赛的基础上，我们需要生成一个关于比赛的自然总结。当然，这可以简单到从训练数据中找到新比赛的最佳匹配统计并检索相应的摘要。但是，有更复杂和优雅的文本生成方式。  
&emsp;&emsp;如果我们要将机器学习结合起来生成自然语言，则可能会执行一系列操作，例如预处理文本，分词，特征工程，训练和预测。   
&emsp;&emsp;**预处理**文本涉及很多操作，例如词干提取（例如，转换listened为listen）和删除标点符号（例如，！和;），以减少词汇量（即特征），从而减少内存需求。重要的是要明白词干提取不是一项微不足道的操作。可能看起来词干提取是一个简单的操作，它依赖于一系列简单的规则，例如从动词中删除ed（例如，listened的词干提取结果是listen）;但是，开发一个优秀的词干提取算法不仅仅是依赖一个简单的规则库，因为对某些特定词提取词干可能很棘手（例如，argued词干提取结果是argue）。此外，词干提取算法的效果可能因语言不同而差异很大。  
&emsp;&emsp;**分词**是可能需要执行的另一个预处理步骤。分词是将语料库划分为小实体（例如，单词）的过程。对于像英语这样的语言来说，这可能显得微不足道，因为这些词是分割的; 但是，对于某些语言（如泰语，日语和中文）而言，情况并非如此，因为这些语言并没有固有的划分。  
&emsp;&emsp;**特征工程**用于将原始文本数据转换为更吸引人的数字格式，因为模型可以基于这种数据格式进行训练，例如，将文本转换为词袋表示或使用n-gram表示（我们将在稍后讨论）。但是请记住，当今最先进的经典模型依赖于更复杂的特征工程技术。以下是几个特征工程技术：  
* **词袋**：这是一种特征工程技术，可根据单词出现频率创建特征表示。 例如，让我们考虑以下句子：  
Bob went to the market to buy some flowers  
Bob bought the flowers to give to Mary  
这两句话的词汇表是：\["Bob", "went", "to", "the", "market", "buy", "some", "flowers", "bought", "give", "Mary"]  
接下来，我们为每一个句子生成一个V（词汇表长度）长度的特征向量，来表示词汇表中的词在句子中的出现次数。在这个例子中，两句话的特征向量是：  
\[1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 0]  
\[1, 0, 2, 1, 0, 0, 0, 1, 1, 1, 1]  
词袋方法的一个关键缺陷是，由于不再保留单词的顺序，它会丢失上下文信息。
* **n-gram**：这是另一种特征工程技术，它将文本分解为由n个字母（或单词）组成的较小组件。 例如，2-gram会将文本分成两个字母（或两个字）的实体。 例如，考虑这句话：  
Bob went to the market to buy some flowers  
这个句子的字母级别的n-gram分解如下：  
\["Bo", "ob", "b ", " w", "we", "en", ..., "me", "e "," f", "fl", "lo", "ow", "we", "er", "rs"]  
单词级别的n-gram分解如下：  
\["Bob went", "went to", "to the", "the market", ..., "to buy", "buy some","some flowers"]  
这种表示（字母级别）的优点是词汇量将比我们使用单词作为大型语料库的特征时要小得多。  
接下来，我们需要构建我们的数据，以便能够将其提供给模型学习。 例如，我们将使用以下形式的数据元组（统计信息，解释统计信息的短语）：  
Total goals = 4, "The game was tied with 2 goals for each team at the end of the first half"  
Team 1 = Manchester United, "The game was between Manchester United and Barcelona"  
Team 1 goals = 5, "Manchester United managed to get 5 goals"  
  
&emsp;&emsp;**学习过程**可能包括三个子模块：**Hidden Markov Model（HMM）**，句子规划模块，语言规划模块。在我们的例子中，HMM可以通过分析相关短语的语料库来学习语言的形态结构和语法属性。 更具体地说，我们将连接数据集中的每个短语以形成一个序列，其中第一个元素是统计量，后跟解释它的短语。 然后，我们跟进当前序列训练一个HMM模型，并预测下一个单词。 具体地说，我们首先将统计数据输入HMM，继而得到HMM的预测; 然后，我们将最后一个预测连接到当前序列，并要求HMM给出另一个预测，依此类推。这将使HMM能够在给定统计数据的情况下输出有意义的短语。  
&emsp;&emsp;接下来，我们可以有一个句子规划器来纠正我们可能在短语中出现的任何语言错误（例如，形态学或语法）。例如，一个句子规划器输出这个短语， _I go house as I go home_; 它可以使用规则数据库，其中包含传达意义的正确方法（例如，需要在动词和单词  _house_ 之间加介词）。  
&emsp;&emsp;现在，我们可以使用HMM为给定的统计数据集生成一组短语。然后，我们需要聚合这些短语，使得短语组成的文章是人类可读的并且正确顺序的。 例如，考虑三个短语， _巴塞罗那队的10号球员在下半场打入一球，巴塞罗那队对阵曼联队，而曼联队的3号球员在上半场得到了一张黄牌_; 按此顺序排列这些句子没有多大意义。 我们希望按顺序排列： _巴塞罗那对阵曼联，曼联队的球员3在上半场得到一张黄牌，而巴塞罗那队的10号球员在下半场得分。_ 为此，我们使用话语规划器; 话语规划器可以安排和构建一组需要传达的信息。   
&emsp;&emsp;现在我们可以获得一组任意测试统计数据，并通过遵循前面的工作流程获得解释统计数据的文章，如图1.3所示：
![image](https://github.com/jiaojunming/Natural-Language-Processing-with-TensorFlow-CN/blob/master/image/ch1_3.jpg)
&emsp;&emsp;在这里，重要的是要注意这是一个非常泛化的解释，仅涵盖最有可能包含在经典NLP方式中的主要通用组件。 根据我们致力于解决的具体应用，细节可能会有很大差异。 例如，某些任务可能需要其他特定于应用程序的关键组件（机器翻译中的规则集合对齐模型）。 然而，在本书中，我们并没有强调这些细节，因为这里的主要目标是讨论更前沿的自然语言处理方式。
### 经典算法的不足
&emsp;&emsp;让我们列出传统方法的几个主要缺点，因为这将为讨论深度学习的动机奠定良好的基础：
* 传统NLP中使用的预处理步骤强制权取舍衡嵌入在文本中的潜在有用信息（例如，标点符号和时态信息），以便通过减少词汇量来使学习成为可能。 尽管预处理仍然在前沿的深度学习解决方案中使用，但由于深度网络具有较大的表达能力，因此预处理并不像传统的NLP工作流程那样重要。
* 特征工程需要人工手动执行。为了设计可靠的系统，需要设计出优秀的特征。 由于需要广泛探索不同的特征空间，因此这个过程非常繁琐。 此外，为了高效的发现鲁棒性高的特征，需要特定业务领域的专业知识，而这对于某些NLP任务来说可能是很难获得的。
* 经典算法需要各种外部资源才能表现良好，而现代并没有多少免费提供的外部资源。 这种外部资源通常是存储在大型数据库中的人工创建的信息。为特定任务创建一个任务可能需要几年时间，具体取决于任务的困难程度（例如，机器翻译规则库）。
## NLP的深度学习解法
&emsp;&emsp;毫不讳言，深度学习彻底改变了机器学习，特别是在计算机视觉、语音识别等领域，当然还有NLP。 深度模型在机器学习的许多领域中创造了一系列范式转换，因为深度模型从原始数据中学习了丰富的特征，而不是使用有限的人工设计特征。 这导致令人讨厌且昂贵的特征工程被淘汰。正因如此，深度模型使传统的工作流程更加高效，因为深层模型同时执行特征学习和任务学习。此外，由于深度模型中存在的大量参数（即权重），它拥有比人类设计的多得多的特征。 然而，由于模型的可解释性差，深度模型被认为是黑盒子。例如，深度模型对于给定问题的特征学习过程中，特征是如何得到的以及特征是什么仍然是悬而未决的问题。  
&emsp;&emsp;深度模型本质上是一种人工神经网络，其具有输入层，中间有许多连接的隐藏层，最后是输出层（例如，分类器或回归器）。 正如你看到的，这就形成了从原始数据到预测的端到端模型。中间的这些隐藏层为深层模型提供了从原始数据中学习优秀特征的能力，最终成功完成当前任务。
### 深度学习的历史
&emsp;&emsp;让我们简要讨论深度学习的根源以及它是如何演变成一种非常有前景的机器学习技术。1960年，Hubel和Weisel进行了一项有趣的实验，发现猫的视觉皮层由简单而复杂的细胞组成，并且这些细胞按层级结构被组织起来。 而且，这些细胞对不同的刺激作出不同的反应。例如，简单单元由不同方向的边缘激活，而复杂单元对空间变化（例如，边缘的方向）不敏感。 这激发了在机器中复制类似行为的动机，从而产生了深度学习的概念。    
&emsp;&emsp;在随后的几年中，神经网络引起了许多研究人员的关注。 1965年，由Ivakhnenko和其他人推出了一种神经网络，该方法基于Rosenblatt著名的感知机，通过一种称为数据分组处理方法（**Group Method of Data Handling，GMDH**）的方法进行训练。 在随后的几年中，神经网络引起了许多研究人员的关注。 1965年，由Ivakhnenko和其他人推出了一种神经网络，该方法基于Rosenblatt著名的感知机（Perceptron），通过一种称为数据分组处理方法（GMDH）的方法进行训练。 后来，在1979年，Fukushima引入了神经认知机（Neocognitron），这为最着名的深度模型 - 卷积神经网络之一奠定了基础。 与始终采用一维输入的感知器不同，神经认知机能够使用卷积操作处理2D输入。  
&emsp;&emsp;
